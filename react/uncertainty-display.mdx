# Displaying Uncertainties

## Overview

AI-generated data varies in reliability. By quantifying model confidence, users can distinguish between reliable outputs and those requiring review. Uncertainty is derived from internal metrics like **log probabilities** (logprobs).

## How it works

### Extracting Confidence Scores

Models like OpenAI’s GPT-series provide log probabilities for their generated tokens. These logprobs can be aggregated to estimate how certain the model is about a given field’s value. Lower logprob values indicate higher uncertainty.
While raw log probabilities are useful diagnostically, they’re not intuitive for end-users. The system converts these logprob values into normalized uncertainty scores (e.g., a 0-1 scale), where higher numbers represent higher uncertainty. 

### Visual Indicators

Fields above a threshold are flagged visually using:

```typescript
type UncertaintyVariant =
   | "none"              // No indicator
   | "underline-close"   // Subtle dotted underline
   | "underline-distant" // Prominent dotted underline
   | "left-border-close" // Subtle left border
   | "left-border-distant" // Prominent left border
   | "blur-small"        // Light blur
   | "blur-large"        // Heavy blur
   | "outline-small"     // Subtle outline
   | "outline-large"     // Prominent outline
```
## Benefits

- **Trust**: Transparency in model confidence builds user trust.
- **Targeted Manual Review**: Instead of reviewing every field, users can focus their attention on uncertain data points, saving time and reducing the overall workload.


