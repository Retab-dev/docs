Retab uses a credit-based pricing system for AI model usage. Different models have different credit costs based on their capabilities and performance characteristics.

## Credit price

`1 Credit = 0.01$`



## Model Pricing

| Model Family | Model Variant | Credits | Tier |
|--------------|---------------|---------|------|
| **GPT-4.1** | nano | 0.1 | Micro/Nano |
| | mini | 0.5 | Small/Mini |
| | base | 2.0 | Large/Advanced |
| **Gemini** | flash-lite | 0.1 | Micro/Nano |
| | flash | 0.5 | Small/Mini |
| | pro | 2.0 | Large/Advanced |
| **OpenAI Reasoning** | o3 | 5.0 | Advanced Reasoning |
| **Auto Models** | auto-micro | 0.1 | Micro/Nano |
| | auto-small | 0.5 | Small/Mini |
| | auto-large | 2.0 | Large/Advanced |


## Extract API Pricing

### Pricing Formula

The total cost for a Retab request is calculated as:

```
Total Credits = preprocessing_cost + (n_consensus × model_credits)
```

### Credit Tiers

- **0.1 credits**: Micro/Nano models (fastest, most efficient)
- **0.5 credits**: Small/Mini models (balanced performance)
- **2.0 credits**: Large/Advanced models (highest capability)
- **5.0 credits**: Reasoning models (highest tier)


Where:
- **preprocessing_cost**: 
  - **0 credits**: For text-based documents (PDF with text, JSON, CSV, etc.)
  - **0.5 credits**: For image-based documents requiring OCR (scanned PDFs, images, etc.)
- **n_consensus**: Number of consensus runs (typically 1-5, depending on your accuracy requirements)
- **model_credits**: The credit cost of the specific model you're using (see table above)


### Examples

**Example 1: Text PDF extraction with GPT-4.1-Mini**
- preprocessing_cost: 0 credits (text-based PDF)
- Model usage: 1 consensus × 0.5 credits = 0.5 credits
- **Total: 0.5 credits**

**Example 2: Scanned document with Gemini-2.5-Pro (3 consensus)**
- preprocessing_cost: 0.5 credits (image-based document requiring OCR)
- Model usage: 3 consensus × 2.0 credits = 6.0 credits
- **Total: 6.5 credits**

**Example 3: JSON extraction with Auto-Micro**
- preprocessing_cost: 0 credits (text-based format)
- Model usage: 1 consensus × 0.1 credits = 0.1 credits  
- **Total: 0.1 credits**

**Example 4: Scanned invoice with Auto-Micro**
- preprocessing_cost: 0.5 credits (image requiring OCR)
- Model usage: 1 consensus × 0.1 credits = 0.1 credits
- **Total: 0.6 credits**

### Model Selection Guide

**Choose Micro/Nano models (0.1 credits)** when:
- You need fast, efficient processing
- Working with simple extraction tasks
- Cost efficiency is the primary concern

**Choose Small/Mini models (0.5 credits)** when:
- You need balanced performance and cost
- Working with moderate complexity tasks
- Good balance of speed and capability

**Choose Large/Advanced models (2.0+ credits)** when:
- You need maximum capability and accuracy
- Working with complex reasoning tasks
- Quality is more important than cost

**Choose Reasoning models (5.0+ credits)** when:
- You need advanced logical reasoning
- Working with complex problem-solving tasks
- Maximum intelligence is required

## Parse API Pricing

### Pricing Formula

The total cost for a parse request is calculated as:

```
Total Credits = model_credits
```

The Parse API follows the same pricing structure as extration:
- **0 credits**: For text-based documents
- **model_credits**: The credit cost of the specific model you're using (see table above)

### Examples

**Example 1: PDF parsing with GPT-4.1-Mini**
- Model usage: 0.5 credits
- **Total: 0.5 credits**

**Example 2: Scanned document with Gemini-flash-lite-2.5**
- Model usage: 0.1 credits
- **Total: 0.1 credits**

**Example 3: JSON parsing with Auto-Micro**
- Model usage: 0.0 credits
- **Total: 0.0 credits**

**Example 3: Text parsing with Auto-Small**
- Model usage: 0.0 credits
- **Total: 0.0 credits**